I have been trying gathering information about ML, AI research in India, especially at IITs and IISc. It appears from a 
[recent post on LinkedIn by Prof Smruti Sarangi](https://www.linkedin.com/posts/smruti-sarangi-1120286_how-a-false-love-for-aiml-is-destroying-ugcPost-6890555621486292992-Oc0r) 
that at least 80% of current Ph. D. aspirants are pursuing research in ML, and AI. Until 2000, PhDs in Soft Computing were mostly from ISI Kolkata. I 
believe ISI produced about 10-15 odd Ph. Ds every year around the time. IITs looked for people in core 
areas of computer science. If I am not mistaken, very few softcomputing specialists feature in CS 
faculty at IITs or IISc.  One of our senior colleagues once remarked that ISI Ph. Ds are good researchers and published a lot. However, their research
is too unidimensional for a CS dept at IITs. I am not sure if he may want to revise his opinion about the recent stream of Ph. Ds from IITs with 80% are in ML, AI and Data Science. 

I fully concur with Prof Sarangi's observations about the knowledge of candidates in the core areas of computer science and Engineering. Out of many 
candidates I have interviewed in various selection committees not even 10% have basic understanding of programming and algorithms. In fact, In one of 
the selection committees at a NIT, one of the panelists from dared to ask a candidate to write
print hello program in C. I was ashtonished and asked the said panelist in inaudible voice: "Are you serious?". She was very confident, and replied back 
in equally inaudible voice: "Just wait and see".  One other independent panel member (I believe a Prof in mechanincal engieering from IITB) was
curiously watching our exchanges.
The candidate did not have any clue of C programming though he claimed to have written programs in C for his dissertation. After the candidate went out, 
the panel member from IITB told me that he knew that the lady panelist would be right. Now it seems IITs and IISc have joined the stream.  
Forget network, database, computer architecture, compilers or OS, the candidates don't even have good background on fundamentals like DS, 
Algorithms, and Discrete Structures.

I will add Data Science to ML-AI craze.  Data scientists are "Stasticians masquareding as computer programmers" or "Programmers masquerading as 
stasticians". However, I believe some of the readers' reactions to Prof Sarangi's post may be valid on the surface. One guy from some industry (possibly 
from gaming and entertainment business) comments that AI and ML provide higher order of abstractions which can aid in more complex problem solving. He 
cites the complexity of ray tracing in discovering gaming alternatives which AI can easily do. He also goes on to say that mechaninical engieers, 
biologists need not understand intricacies of neural networks to solve their problems. But the question that remains unanswered is: 

- How does the mechnical engineer or the biologist even know that the solution is correct? 

If none of them are trained in their core competence then they will also lack the ability to even judge a solution. So the real risk of AI is not 
in super intelligent machnines controling humans but in machine that are not smart enough to perform the tasks that humans give them.
The other side of spectrum is humans turn dumb when they think some software tools could make smarter decision on their behalf.  
MATLAB is one such example. Electrical engineers have reasons to use MATLAB. Power and controls engineers use MATLAB a lot. But a 
solid understanding of the core is required to use the tools intelligently.  Possibly, those who have a different point of view have not been able to
get to the bottom of the main theme of Prof Sarangi's post. He is no ameture in the area of AI-ML. He does have a high regard for ML and AI techniques. 
Like him, many other researchers also use AI and techniques based on heuristics and learning for their research. 

David L. Parans, Prof Emeritus of McMaster University published a note on 
["Real Risk of AI"](https://cacm.acm.org/magazines/2017/10/221330-the-real-risks-of-artificial-intelligence/fulltext). He had early training in AI as a
student from Professor who were pioneers in the field of AI research. He says that AI relied more on intutitions than disciplined approach to problem 
solving that is practiced in physics, mathematics or engineering. In a discipilined aproach, a problem is thoroughly analyzed, mathematics and physics
behnid the problem is unraveled, the solutions are often hand simulated, and furthermore assertion-based model checkers are used for verification. 
He goes on to say that his Professors at CMU were "clever but had cavalier attitude" to 
specfic questions and recommended the students to "try and fix it". The foundation of AI research is based on mimicking human intuition and
ptoblem solving through heuritics. However, a heuristics works if it is used to select one from possible alternatives or to determine presentation
order. More frequently heuristics are used for speeding up the searching of solution space applying intelligent prunning and feasibility measures. 
In other situation heuristics are untrustworthy. Though Prof Sarangi's post is a bit blunt to taste of AI 
and ML enthusiast, David Pranas puts the same thing using "higher order of abstractions". He states that there are three different ways of AI 
research:

- Building programs that imitate human behavior in order to understand human thinking;
- Building programs that play games well; and
- Showing that practical computerized products can use the methods that humans use.
 
The first one tries to model human brain where certain elements are unexplained (not understood) and dublicated as black boxes. He recognizes that 
"writing game-playing programs is harmless and builds capabilities". However, it may be dangerous if practical products mimics human methods. He cites
the example of Captch based recognitions work because the character recoginition has not be solved prorammatically. The 
programs that imitating humans intuitions are not always the best way of problem solving with computers. In fact imitating human intuition could make
programs untrustworthy and dangerous. He gives a set of examples to conclude that his impressions about AI techniques are old, however, the lessons they 
taught him remain relevant even today. 
    
One may ask: how it has destoyed engineering education in general and CSE in particular? The problem lies in how the lessions are conducted. If teachers
have not written much of code themselves, their expectation from the students would be low. In fact, many faculty members who
teach Data Structure course rely on teaching theory and complexity analysis and do not give programming assignments. The other problem in checking
programming assignment is an ardous task. Rampant copies from Internet sources make assignment useless. I used moss to check copies, and found about 
30-35% students indulge in copying. It becomes a serious problem to deal with the copies. Most students deny their involvement in copying or 
assisting friends to copy. The explanations were crazy and outrightly false. Sometimes, the problem goes down to acute interpersonal bickerings among
TAs and students. Quite obviously, no one likes to be a part of it. So, the alternative is not to bother about checking assignments.  

Someone writes that an average CS UG students at IITs do write 50k lines of code before graduating out. Probably, he must be talking about the situation
twenty five years back, when the Internet resources were not available that easily. We used to give rigorous programming practice to the undergraduate students. The class size used to be small. Any copy in assignments can be detected easily. An average UG students may write about 25-30K lines of C code 
if not more. Most system courses like OS, Databases, Networks, Compilers would require each student to write in an average 10k lines of C code. 
Even in Data Structures course one would typically do a project that often required 5-7k lines of code. It is not true these days. I remember
having spent two semesters at one of the older IITs where I taught Design of Algorithms in one semester. I asked the students to write programs for a 
hypothetical coin game and test their strategies by conducting tournaments among the program creaters. We had a program to play out one student's 
program against the other. The idea was to train the students in discovering better strategies in a competitive environment. The entire class 
refused to take part in the tournament based assignment. Of course, one of the major argument was that it would vitiate the camadrie among batch mates.

As the Internet resources became widely available, most students resorted to copy and paste kind of approach to problem solving. I used MOSS to 
check copying by downloading codes from Geeks for Geeks, Programiz, Rosetta, etc and including them as assignments. Invariably 30% of the student
who submitted assigment copied. About 15% did not bother to submit assignments. They estimated the time to spend in coding can pay off better if
they only work for their exams. I noticed a comment on Prof Sarangi's post that IIT graduates 25 years back used to write 50k lines of code in 
an average. Someone else tells that it is stupid to ask a Ph. D. student how many lines of code he has written. I am not sure if many of these readers
are in timewraps. Most of them are successful today because they received training 20-25 years back. The only tricks that works today is an element of 
surprise. For example, we used whatsapp based exam for remote conduct of exam. We created a protocol which had full of
surprises for students in the beginning of COVID-19 lockdown. Now I understand, that students use two computers simultaneously to search for
solution and one for sending solutions. Thinking and learning goes to the basics of the understanding. 

It is unfortunate craze promoted by industry honcos to create fissures in foundations of engineering education. I am not sure if industries are only 
driven by short term gains or see AI, ML and Data Science can really propel industry 4.0.
